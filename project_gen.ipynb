{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUhTVMYboWA9"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XjYnuscoc4E"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1eiDE3Rn7_B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import jieba\n",
        "import random\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from transformers import BertTokenizer, BartForConditionalGeneration, Text2TextGenerationPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0A9jI7ppJ83"
      },
      "outputs": [],
      "source": [
        "def read_file(file_p):\n",
        "    out_arr = []\n",
        "    with open(file_p, encoding='utf-8') as f:\n",
        "        out_arr = [x.strip() for x in f.readlines()]\n",
        "    return out_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbbRWEFOQkiP"
      },
      "outputs": [],
      "source": [
        "def get_task_processor(task, data_dir):\n",
        "    \"\"\"\n",
        "    A TSV processor for stsa, trec and snips dataset.\n",
        "    \"\"\"\n",
        "    \n",
        "    if task == 'tnews':\n",
        "        return TSVDataProcessor(data_dir=data_dir, skip_header=False, label_col=0, text_col=1)\n",
        "    elif task == 'chnsenticorp':\n",
        "        return TSVDataProcessor(data_dir=data_dir, skip_header=False, label_col=0, text_col=1)\n",
        "    else:\n",
        "        raise ValueError('Unknown task')\n",
        "    \n",
        "\n",
        "def get_data(task, data_dir, data_seed=159):\n",
        "    random.seed(data_seed)\n",
        "    processor = get_task_processor(task, data_dir)\n",
        "\n",
        "    examples = dict()\n",
        "\n",
        "    examples['train'] = processor.get_train_examples()\n",
        "    examples['dev'] = processor.get_dev_examples()\n",
        "    examples['test'] = processor.get_test_examples()\n",
        "\n",
        "    for key, value in examples.items():\n",
        "        print('#{}: {}'.format(key, len(value)))\n",
        "    return examples, processor.get_labels(task)\n",
        "\n",
        "\n",
        "class InputExample:\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text, label=None):\n",
        "        self.guid = guid\n",
        "        self.text = text\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return [self.input_ids, self.input_mask,\n",
        "                self.segment_ids, self.label_id][item]\n",
        "\n",
        "\n",
        "class DatasetProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_test_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self, task_name):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "class TSVDataProcessor(DatasetProcessor):\n",
        "    \"\"\"Processor for dataset to be augmented.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, skip_header, label_col, text_col):\n",
        "        self.data_dir = data_dir\n",
        "        self.skip_header = skip_header\n",
        "        self.label_col = label_col\n",
        "        self.text_col = text_col\n",
        "\n",
        "    def get_train_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"dev.tsv\")), \"dev\")\n",
        "\n",
        "    def get_test_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"test.tsv\")), \"test\")\n",
        "\n",
        "    def get_labels(self, task_name):\n",
        "        \"\"\"add your dataset here\"\"\"\n",
        "        labels = set()\n",
        "        with open(os.path.join(self.data_dir, \"train.tsv\"), \"r\") as in_file:\n",
        "            for line in in_file:\n",
        "                labels.add(line.split(\"\\t\")[self.label_col])\n",
        "        return sorted(labels)\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if self.skip_header and i == 0:\n",
        "                continue\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text = line[self.text_col]\n",
        "            label = line[self.label_col]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text=text, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4JABa1E88vW"
      },
      "outputs": [],
      "source": [
        "class BertAugmentor(object):\n",
        "  def __init__(self, model_dir='bert-base-chinese', beam_size=3):\n",
        "    self.beam_size = beam_size\n",
        "    self.model = pipeline('fill-mask', model=model_dir, top_k=beam_size)\n",
        "    self.mask_token = self.model.tokenizer.mask_token\n",
        "\n",
        "  def gen_sen(self, query, num_mask):\n",
        "    '''{'sequence': ,'score' }'''\n",
        "    tops = self.model(query)[0] if num_mask > 1 else self.model(query)\n",
        "    num_mask -= 1\n",
        "    while num_mask:\n",
        "      qs = [x['sequence'] for x in tops]\n",
        "      new_tops = self.model(qs)[0] if num_mask > 1 else self.model(qs)\n",
        "      cur_tops = []\n",
        "      for q, q_preds in zip(tops, new_tops):\n",
        "        pre_score = q['score']\n",
        "        for each in q_preds:\n",
        "          each['cur_score'] = each['score']\n",
        "          each['score'] = pre_score * each['score']\n",
        "          cur_tops.append(each)\n",
        "      tops = sorted(cur_tops, key=lambda x: x['score'], reverse=True)[:self.beam_size]\n",
        "      num_mask -= 1\n",
        "    return tops\n",
        "\n",
        "  def word_replacement(self, query, n):\n",
        "    label_len = len(query.label)\n",
        "    out_arr = []\n",
        "    aug_list = []\n",
        "    seg_list = jieba.cut(query.text, cut_all=False)\n",
        "    seg_list = [x for x in seg_list]\n",
        "    set_index = [i + 2 for i, _ in enumerate(seg_list)]\n",
        "    set_index.pop(0)\n",
        "    seg_list = [query.label] + [\" \"] + seg_list\n",
        "    # randomly sample n index to replace\n",
        "    replace_index = random.sample(set_index, min(n, len(set_index)))\n",
        "    for cur_index in replace_index:\n",
        "      new_query = seg_list.copy()\n",
        "      word_len = len(new_query[cur_index])\n",
        "      new_word = [self.mask_token] * word_len\n",
        "      new_query[cur_index] = ''.join(new_word)\n",
        "      gen_qs = self.gen_sen(''.join(new_query), word_len)\n",
        "      out_arr.extend(gen_qs)\n",
        "    out_arr = sorted(out_arr, key=lambda x: x['score'], reverse=True)[:n]\n",
        "    for seq in out_arr:\n",
        "      x = seq['sequence']\n",
        "      x = re.sub(\"(?<![ -~]) (?![ -~])\", '', x)\n",
        "      x_label, x_text = x[:label_len], x[label_len:]\n",
        "      aug_list.append([x_label, x_text])\n",
        "    return aug_list\n",
        "\n",
        "  def word_insertion(self, query, n):\n",
        "    label_len = len(query.label)\n",
        "    out_arr = []\n",
        "    aug_list = []\n",
        "    seg_list = jieba.cut(query.text, cut_all=False)\n",
        "    seg_list = [x for x in seg_list]\n",
        "    set_index = [i + 3 for i, _ in enumerate(seg_list)]\n",
        "    seg_list = [query.label] + [\" \"] + seg_list\n",
        "    # randomly sample n index to replace\n",
        "    insert_index = random.sample(set_index, min(n, len(set_index)))\n",
        "    # randomly insert [MASK] between words\n",
        "    for cur_index in insert_index:\n",
        "      new_query = seg_list.copy()\n",
        "      # randomly insert n characters with 1<=n<=3\n",
        "      insert_num = np.random.randint(1, 4)\n",
        "      for _ in range(insert_num):\n",
        "        new_query.insert(cur_index, self.mask_token)\n",
        "      gen_qs = self.gen_sen(''.join(new_query), insert_num)\n",
        "      out_arr.extend(gen_qs)\n",
        "    out_arr = sorted(out_arr, key=lambda x: x['score'], reverse=True)[:n]\n",
        "    for seq in out_arr:\n",
        "      x = seq['sequence']\n",
        "      x = re.sub(\"(?<![ -~]) (?![ -~])\", '', x)\n",
        "      x_label, x_text = x[:label_len], x[label_len:]\n",
        "      aug_list.append([x_label, x_text])\n",
        "    return aug_list\n",
        "\n",
        "  def aug(self, query, num_aug=9):\n",
        "    num_new_per_technique = int(num_aug / 2)\n",
        "    augmented_sentences = self.word_replacement(query, num_new_per_technique)\n",
        "    augmented_sentences += self.word_insertion(query, num_new_per_technique+2)\n",
        "    return augmented_sentences\n",
        "  \n",
        "  def augment(self, example, num_aug=9, aug_train=True):\n",
        "    if aug_train:\n",
        "      out = open(data_dir + '/train_aug.tsv', 'w')\n",
        "    else:\n",
        "      out = open(data_dir + '/dev_aug.tsv', 'w')\n",
        "    out_writer = csv.writer(out, delimiter='\\t')\n",
        "    for query in example:\n",
        "      for i in self.aug(query, num_aug):\n",
        "        out_writer.writerow([i[0], i[1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1Cj-Q71VpT1S"
      },
      "outputs": [],
      "source": [
        "for exp_id in range(10):\n",
        "    data_dir = os.path.join(\"/content/gdrive/My Drive/project/tnews\", \"exp_{}\".format(exp_id))\n",
        "    processor = get_task_processor('tnews', data_dir)\n",
        "    train_examples = processor.get_train_examples()\n",
        "    # model = BertAugmentor(model_dir = data_dir+'/model')\n",
        "    model = BertAugmentor()\n",
        "    model.augment(train_examples)\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMj+hK6YnyOyLwYRKquoy6r"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}