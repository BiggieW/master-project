{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91ijzC4c0tEG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HogrEULA5XcU"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==2.9.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEZWOqCv2KrM"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import csv\n",
        "import torch\n",
        "import jieba\n",
        "import argparse\n",
        "\n",
        "import random\n",
        "from transformers import BertTokenizer\n",
        "from transformers.modeling_bert import BertForSequenceClassification\n",
        "\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8v6jCy-05Nb"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_lIYdxD4_q9"
      },
      "outputs": [],
      "source": [
        "def get_task_processor(task, data_dir):\n",
        "    \"\"\"\n",
        "    A TSV processor for stsa, trec and snips dataset.\n",
        "    \"\"\"\n",
        "    if task == 'tnews':\n",
        "      return TSVDataProcessor(data_dir=data_dir, skip_header=False, label_col=0, text_col=1)\n",
        "    elif task == 'chnsenticorp':\n",
        "        return TSVDataProcessor(data_dir=data_dir, skip_header=False, label_col=0, text_col=1)\n",
        "    else:\n",
        "        raise ValueError('Unknown task')\n",
        "\n",
        "\n",
        "def get_data(task, aug_type, data_dir, aug_train = False, aug_dev = False, data_seed=159):\n",
        "    random.seed(data_seed)\n",
        "    processor = get_task_processor(task, data_dir)\n",
        "\n",
        "    examples = dict()\n",
        "    if not aug_train:\n",
        "      examples['train'] = processor.get_train_examples()\n",
        "    else:\n",
        "      if aug_type == 'pbert':\n",
        "        examples['train'] = processor.get_train_p_aug_examples()\n",
        "      elif aug_type == 'cbert':\n",
        "        examples['train'] = processor.get_train_c_aug_examples()\n",
        "      elif aug_type == 'eda':\n",
        "        examples['train'] = processor.get_train_e_aug_examples()\n",
        "\n",
        "    if aug_dev:\n",
        "      examples['dev'] = processor.get_dev_aug_examples()\n",
        "    else:\n",
        "      examples['dev'] = processor.get_dev_examples()\n",
        "   \n",
        "    examples['test'] = processor.get_test_examples()\n",
        "\n",
        "    for key, value in examples.items():\n",
        "        print('#{}: {}'.format(key, len(value)))\n",
        "    return examples, processor.get_labels(task)\n",
        "\n",
        "\n",
        "class InputExample:\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return [self.input_ids, self.input_mask,\n",
        "                self.segment_ids, self.label_id][item]\n",
        "\n",
        "\n",
        "class DatasetProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_test_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self, task_name):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "class TSVDataProcessor(DatasetProcessor):\n",
        "    \"\"\"Processor for dataset to be augmented.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, skip_header, label_col, text_col):\n",
        "        self.data_dir = data_dir\n",
        "        self.skip_header = skip_header\n",
        "        self.label_col = label_col\n",
        "        self.text_col = text_col\n",
        "\n",
        "    def get_train_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"train.tsv\")), \"train\")\n",
        "    \n",
        "    def get_train_p_aug_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"train_aug.tsv\")), \"train\")\n",
        "    \n",
        "    def get_train_c_aug_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"cbert_aug.tsv\")), \"train\")\n",
        "        \n",
        "    def get_train_e_aug_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"eda_aug.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"dev.tsv\")), \"dev\")\n",
        "    \n",
        "    def get_dev_aug_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"dev_aug.tsv\")), \"dev\")\n",
        "\n",
        "    def get_test_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"test.tsv\")), \"test\")\n",
        "\n",
        "    def get_labels(self, task_name):\n",
        "        \"\"\"add your dataset here\"\"\"\n",
        "        labels = set()\n",
        "        with open(os.path.join(self.data_dir, \"train.tsv\"), \"r\") as in_file:\n",
        "            for line in in_file:\n",
        "                labels.add(line.split(\"\\t\")[self.label_col])\n",
        "        return sorted(labels)\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if self.skip_header and i == 0:\n",
        "                continue\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text_a = line[self.text_col]\n",
        "            seg_list = jieba.cut(text_a, cut_all=False)\n",
        "            seg_list = [x for x in seg_list]\n",
        "            text_a = ' '.join(seg_list)\n",
        "            label = line[self.label_col]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdFNJtSb4ds2"
      },
      "outputs": [],
      "source": [
        "BERT_MODEL = 'hfl/chinese-roberta-wwm-ext'\n",
        "class Classifier:\n",
        "    def __init__(self, label_list, device, cache_dir):\n",
        "        self._label_list = label_list\n",
        "        self._device = device\n",
        "\n",
        "        self._tokenizer = BertTokenizer.from_pretrained(BERT_MODEL,\n",
        "                                              do_basic_tokenize=False,\n",
        "                                              do_lower_case=True,\n",
        "                                              cache_dir=cache_dir)\n",
        "\n",
        "        self._model = BertForSequenceClassification.from_pretrained(BERT_MODEL,\n",
        "                                                                    num_labels=len(label_list),\n",
        "                                                                    cache_dir=cache_dir)\n",
        "        self._model.to(device)\n",
        "\n",
        "        self._optimizer = None\n",
        "\n",
        "        self._dataset = {}\n",
        "        self._data_loader = {}\n",
        "\n",
        "    def load_data(self, set_type, examples, batch_size, max_length, shuffle):\n",
        "        self._dataset[set_type] = examples\n",
        "        self._data_loader[set_type] = _make_data_loader(\n",
        "            examples=examples,\n",
        "            label_list=self._label_list,\n",
        "            tokenizer=self._tokenizer,\n",
        "            batch_size=batch_size,\n",
        "            max_length=max_length,\n",
        "            shuffle=shuffle)\n",
        "\n",
        "    def get_optimizer(self, learning_rate, warmup_steps, t_total):\n",
        "        self._optimizer, self._scheduler = _get_optimizer(\n",
        "            self._model, learning_rate=learning_rate,\n",
        "            warmup_steps=warmup_steps, t_total=t_total)\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self._model.train()\n",
        "\n",
        "        for step, batch in enumerate(tqdm(self._data_loader['train'],\n",
        "                                          desc='Training')):\n",
        "            batch = tuple(t.to(self._device) for t in batch)\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2],\n",
        "                      'labels': batch[3]}\n",
        "\n",
        "            self._optimizer.zero_grad()\n",
        "            outputs = self._model(**inputs)\n",
        "            loss = outputs[0]  # model\n",
        "            loss.backward()\n",
        "            self._optimizer.step()\n",
        "            self._scheduler.step()\n",
        "\n",
        "    def evaluate(self, set_type):\n",
        "        self._model.eval()\n",
        "\n",
        "        preds_all, labels_all = [], []\n",
        "        data_loader = self._data_loader[set_type]\n",
        "\n",
        "        for batch in tqdm(data_loader,\n",
        "                          desc=\"Evaluating {} set\".format(set_type)):\n",
        "            batch = tuple(t.to(self._device) for t in batch)\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2],\n",
        "                      'labels': batch[3]}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self._model(**inputs)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            preds_all.append(preds)\n",
        "            labels_all.append(inputs[\"labels\"])\n",
        "\n",
        "        preds_all = torch.cat(preds_all, dim=0)\n",
        "        labels_all = torch.cat(labels_all, dim=0)\n",
        "\n",
        "        return torch.sum(preds_all == labels_all).item() / labels_all.shape[0]\n",
        "    \n",
        "    def analysis(self, set_type):\n",
        "        self._model.eval()\n",
        "\n",
        "        preds_all, labels_all = [], []\n",
        "        data_loader = self._data_loader[set_type]\n",
        "\n",
        "        for batch in tqdm(data_loader,\n",
        "                          desc=\"Evaluating {} set\".format(set_type)):\n",
        "            batch = tuple(t.to(self._device) for t in batch)\n",
        "            inputs = {'input_ids': batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2],\n",
        "                      'labels': batch[3]}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self._model(**inputs)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            preds_all.append(preds)\n",
        "            labels_all.append(inputs[\"labels\"])\n",
        "\n",
        "        preds_all = torch.cat(preds_all, dim=0)+1\n",
        "        labels_all = torch.cat(labels_all, dim=0)+1\n",
        "        all_count = torch.bincount(labels_all)[1:]\n",
        "        correct_labels = (preds_all == labels_all) * preds_all\n",
        "        count = torch.bincount(correct_labels)[1:]\n",
        "        \n",
        "        return 'class, total number of entries, number of correctly classified entries, accuracy' ,list(zip(self._label_list, all_count, count, count/all_count))\n",
        "\n",
        "\n",
        "def _get_optimizer(model, learning_rate, warmup_steps, t_total):\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
        "                                                num_training_steps=t_total)\n",
        "    return optimizer, scheduler\n",
        "\n",
        "\n",
        "def _make_data_loader(examples, label_list, tokenizer, batch_size, max_length, shuffle):\n",
        "    features = convert_examples_to_features(examples,\n",
        "                                            tokenizer,\n",
        "                                            label_list=label_list,\n",
        "                                            max_length=max_length,\n",
        "                                            output_mode=\"classification\")\n",
        "\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n",
        "\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0R1hkYb0_J0"
      },
      "outputs": [],
      "source": [
        "random.seed()\n",
        "torch.backends.cudnn.deterministic = True\n",
        "epochs = 10\n",
        "min_epochs = 0\n",
        "learning_rate = 4e-5\n",
        "warmup_steps = 100\n",
        "batch_size = 15\n",
        "max_seq_length = 64\n",
        "hidden_dropout_prob = 0.1\n",
        "cache = \"transformers_cache\"\n",
        "\n",
        "examples, label_list = get_data(\n",
        "    task='tnews',\n",
        "    aug_type = 'pbert',\n",
        "    data_dir = '/content/gdrive/My Drive/project/tnews/exp_0',\n",
        "    aug_train = False,\n",
        "    aug_dev = False,\n",
        "    data_seed = 42)\n",
        "\n",
        "\n",
        "\n",
        "t_total = len(examples['train']) // epochs\n",
        "\n",
        "classifier = Classifier(label_list=label_list, device=device, cache_dir = cache)\n",
        "classifier.get_optimizer(learning_rate=learning_rate,\n",
        "                          warmup_steps=warmup_steps,\n",
        "                          t_total=t_total)\n",
        "\n",
        "classifier.load_data(\n",
        "    'train', examples['train'], batch_size, max_length=max_seq_length, shuffle=True)\n",
        "classifier.load_data(\n",
        "    'dev', examples['dev'], batch_size, max_length=max_seq_length, shuffle=True)\n",
        "classifier.load_data(\n",
        "    'test', examples['test'], batch_size, max_length=max_seq_length, shuffle=True)\n",
        "\n",
        "print('=' * 60, '\\n', 'Training', '\\n', '=' * 60, sep='')\n",
        "best_dev_acc, final_test_acc = -1., -1.\n",
        "for epoch in range(epochs):\n",
        "    classifier.train_epoch()\n",
        "    dev_acc = classifier.evaluate('dev')\n",
        "\n",
        "    if epoch >= min_epochs:\n",
        "        do_test = (dev_acc > best_dev_acc)\n",
        "        best_dev_acc = max(best_dev_acc, dev_acc)\n",
        "    else:\n",
        "        do_test = False\n",
        "\n",
        "    print('Epoch {}, Dev Acc: {:.4f}, Best Ever: {:.4f}'.format(\n",
        "        epoch, 100. * dev_acc, 100. * best_dev_acc))\n",
        "\n",
        "    if do_test:\n",
        "        final_test_acc = classifier.evaluate('test')\n",
        "        print('Test Acc: {:.4f}'.format(100. * final_test_acc))\n",
        "\n",
        "print('Final Dev Acc: {:.4f}, Final Test Acc: {:.4f}'.format(\n",
        "    100. * best_dev_acc, 100. * final_test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NQXES4C1e42"
      },
      "outputs": [],
      "source": [
        "# print(classifier._label_list)\n",
        "classifier.analysis('test')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyPtoQ/LW/0TfjOM2vFScaIX"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}