{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyP0ClHbE2LR2NkPwJVeb9s6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "oNdeEqVLW_KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7NDuH8npTYv"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==2.9.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import os\n",
        "import jieba\n",
        "import logging\n",
        "import argparse\n",
        "import random\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from transformers.tokenization_bert import BertTokenizer\n",
        "from transformers.modeling_bert import BertForMaskedLM, BertOnlyMLMHead\n",
        "from transformers import pipeline\n",
        "\n",
        "from transformers import AdamW"
      ],
      "metadata": {
        "id": "I82s9jdrp14b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:', device)"
      ],
      "metadata": {
        "id": "lOywMFmApYso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_task_processor(task, data_dir):\n",
        "    \"\"\"\n",
        "    A TSV processor for stsa, trec and snips dataset.\n",
        "    \"\"\"\n",
        "    \n",
        "    if task == 'tnews':\n",
        "        return TSVDataProcessor(data_dir=data_dir, skip_header=False, label_col=0, text_col=1)\n",
        "    elif task == 'chnsenticorp':\n",
        "        return TSVDataProcessor(data_dir=data_dir, skip_header=False, label_col=0, text_col=1)\n",
        "    else:\n",
        "        raise ValueError('Unknown task')\n",
        "\n",
        "\n",
        "def get_data(task, data_dir):\n",
        "    processor = get_task_processor(task, data_dir)\n",
        "\n",
        "    examples = dict()\n",
        "\n",
        "    examples['train'] = processor.get_train_examples()\n",
        "    examples['dev'] = processor.get_dev_examples()\n",
        "    examples['test'] = processor.get_test_examples()\n",
        "\n",
        "    for key, value in examples.items():\n",
        "        print('#{}: {}'.format(key, len(value)))\n",
        "    return examples, processor.get_labels(task)\n",
        "\n",
        "\n",
        "class InputExample:\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text, label=None):\n",
        "        self.guid = guid\n",
        "        self.text = text\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return [self.input_ids, self.input_mask,\n",
        "                self.segment_ids, self.label_id][item]\n",
        "\n",
        "\n",
        "class DatasetProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_test_examples(self):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self, task_name):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "class TSVDataProcessor(DatasetProcessor):\n",
        "    \"\"\"Processor for dataset to be augmented.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, skip_header, label_col, text_col):\n",
        "        self.data_dir = data_dir\n",
        "        self.skip_header = skip_header\n",
        "        self.label_col = label_col\n",
        "        self.text_col = text_col\n",
        "\n",
        "    def get_train_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"dev.tsv\")), \"dev\")\n",
        "\n",
        "    def get_test_examples(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(self.data_dir, \"test.tsv\")), \"test\")\n",
        "\n",
        "    def get_labels(self, task_name):\n",
        "        \"\"\"add your dataset here\"\"\"\n",
        "        labels = set()\n",
        "        with open(os.path.join(self.data_dir, \"train.tsv\"), \"r\") as in_file:\n",
        "            for line in in_file:\n",
        "                labels.add(line.split(\"\\t\")[self.label_col])\n",
        "        return sorted(labels)\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if self.skip_header and i == 0:\n",
        "                continue\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            text = line[self.text_col]\n",
        "            label = line[self.label_col]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text=text, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n"
      ],
      "metadata": {
        "id": "UbbRWEFOQkiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, init_ids, input_ids, input_mask, masked_lm_labels, label_length):\n",
        "        self.init_ids = init_ids\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.masked_lm_labels = masked_lm_labels\n",
        "        self.label_length = label_length\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, max_seq_length, tokenizer): \n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "    features = []\n",
        "    masked_lm_prob = 0.15\n",
        "    max_predictions_per_seq = 20\n",
        "\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        seg_list = jieba.cut(example.text, cut_all=False)\n",
        "        seg_list = [x for x in seg_list]\n",
        "        modified_example = example.label + \" \" + ' '.join(seg_list)\n",
        "        label_len = len(tokenizer.tokenize(example.label))\n",
        "        tokens_a = tokenizer.tokenize(modified_example, truncation=True, max_length=512)\n",
        "        # Account for [CLS] and [SEP] and label with \"(2+label_len)\"\n",
        "        if len(tokens_a) > max_seq_length - (2+label_len):\n",
        "            tokens_a = tokens_a[0:(max_seq_length - (2+label_len))]\n",
        "\n",
        "        # take care of prepending the class label in this code\n",
        "        tokens = []\n",
        "        tokens.append(\"[CLS]\")\n",
        "        for token in tokens_a:\n",
        "            tokens.append(token)\n",
        "        tokens.append(\"[SEP]\")\n",
        "        masked_lm_labels = [-100] * max_seq_length\n",
        "\n",
        "        cand_indexes = [] # word index except label/[cls]/[sep]\n",
        "        for (i, token) in enumerate(tokens):\n",
        "            # making sure that masking of # prepended label is avoided\n",
        "            if token == \"[CLS]\" or token == \"[SEP]\" or (i < label_len + 1):\n",
        "                continue\n",
        "            if (len(cand_indexes) >= 1 and token.startswith(\"##\")):\n",
        "                cand_indexes[-1].append(i)\n",
        "            else:\n",
        "                cand_indexes.append([i])\n",
        "        num_to_predict = min(max_predictions_per_seq, max(1, int(round(len(cand_indexes) * masked_lm_prob))))\n",
        "        masked = random.sample(cand_indexes, num_to_predict)\n",
        "\n",
        "        output_tokens = list(tokens)\n",
        "\n",
        "        for mask in masked:\n",
        "          for i in mask:\n",
        "              masked_lm_labels[i] = tokenizer.convert_tokens_to_ids([tokens[i]])[0]\n",
        "              output_tokens[i] = \"[MASK]\"\n",
        "\n",
        "\n",
        "        init_ids = tokenizer.convert_tokens_to_ids(tokens)             # before masking\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(output_tokens)     # after masking\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            init_ids.append(0)\n",
        "            input_ids.append(0)\n",
        "            input_mask.append(0)\n",
        "\n",
        "        assert len(init_ids) == max_seq_length\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "\n",
        "        features.append(                                      # an example\n",
        "            InputFeatures(init_ids=init_ids,                  # [101,313,233,4556,79,...]\n",
        "                          input_ids=input_ids,                # [101,313,233,103,103,...]  \n",
        "                          input_mask=input_mask,              # [1,1,1,1,1,0,0,0,...]\n",
        "                          masked_lm_labels=masked_lm_labels,  # [-100,-100,273,493,-100,...]\n",
        "                          label_length=label_len))\n",
        "    return features"
      ],
      "metadata": {
        "id": "fFAlZhcfgY8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(features):\n",
        "    all_init_ids = torch.tensor([f.init_ids for f in features], dtype=torch.long)\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_masked_lm_labels = torch.tensor([f.masked_lm_labels for f in features], dtype=torch.long)\n",
        "    all_label_lengths = torch.tensor([f.label_length for f in features],dtype=torch.long)\n",
        "    tensor_data = TensorDataset(all_init_ids, all_input_ids, all_input_mask, all_masked_lm_labels, all_label_lengths)\n",
        "    return tensor_data"
      ],
      "metadata": {
        "id": "3cM1VMHuq6EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_dev_loss(model, dev_dataloader):\n",
        "    model.eval()\n",
        "    sum_loss = 0.\n",
        "    for step, batch in enumerate(dev_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        _, input_ids, input_mask, masked_ids, label_lengths = batch\n",
        "        inputs = {'input_ids': batch[1],\n",
        "                  'attention_mask': batch[2],\n",
        "                  'masked_lm_labels': batch[3]}\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs[0]\n",
        "        sum_loss += loss.item() * dev_dataloader.batch_size\n",
        "    return sum_loss/len(dev_dataloader.dataset)\n",
        "\n",
        "def train_pbert_and_augment(task_name, data_dir, output_dir, max_seq_length, train_batch_size, num_train_epochs, learning_rate, cache):\n",
        "    \n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    processor = get_task_processor(task_name, data_dir)\n",
        "    label_list = processor.get_labels(task_name)\n",
        "\n",
        "    # load train and dev data\n",
        "    train_examples = processor.get_train_examples()\n",
        "    dev_examples = processor.get_dev_examples()\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL,\n",
        "                                              do_basic_tokenize=False,\n",
        "                                              model_max_length=512,\n",
        "                                              cache_dir=cache)\n",
        "\n",
        "    model = BertForMaskedLM.from_pretrained(BERT_MODEL,\n",
        "                                            cache_dir=cache)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # train data\n",
        "    train_features = convert_examples_to_features(train_examples,\n",
        "                                                  max_seq_length,\n",
        "                                                  tokenizer)\n",
        "    train_data = prepare_data(train_features)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler,\n",
        "                                  batch_size=train_batch_size)\n",
        "\n",
        "\n",
        "    # dev data\n",
        "    dev_features = convert_examples_to_features(dev_examples,\n",
        "                                                  max_seq_length,\n",
        "                                                  tokenizer)\n",
        "    dev_data = prepare_data(dev_features)\n",
        "    dev_sampler = SequentialSampler(dev_data)\n",
        "    dev_dataloader = DataLoader(dev_data, sampler=dev_sampler,\n",
        "                                  batch_size=train_batch_size)\n",
        "\n",
        "    num_train_steps = int(len(train_features) / train_batch_size * num_train_epochs)\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_features))\n",
        "    logger.info(\"  Batch size = %d\", train_batch_size)\n",
        "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
        "\n",
        "    # Prepare optimizer\n",
        "    t_total = num_train_steps\n",
        "    no_decay = ['bias', 'gamma', 'beta', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n",
        "\n",
        "    best_dev_loss = float('inf')\n",
        "    for epoch in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
        "        avg_loss = 0.\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            _, input_ids, input_mask, masked_ids, label_lengths = batch\n",
        "            inputs = {'input_ids': batch[1],\n",
        "                      'attention_mask': batch[2],\n",
        "                      'masked_lm_labels': batch[3]}\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "            avg_loss += loss.item()\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "            if (step + 1) % 3 == 0:\n",
        "                print(\"avg_loss: {}\".format(avg_loss / 3))\n",
        "            avg_loss = 0.\n",
        "\n",
        "        # eval on dev after every epoch\n",
        "        dev_loss = compute_dev_loss(model, dev_dataloader)\n",
        "        print(\"Epoch {}, Dev loss {}\".format(epoch, dev_loss))\n",
        "        if dev_loss < best_dev_loss:\n",
        "            best_dev_loss = dev_loss\n",
        "            print(\"Saving model. Best dev so far {}\".format(best_dev_loss))\n",
        "            pipe = pipeline('fill-mask')\n",
        "            pipe.save_pretrained(output_dir+'/model')\n",
        "    "
      ],
      "metadata": {
        "id": "C0B4kcv-rNNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BERT_MODEL = 'hfl/chinese-bert-wwm' #'bert-base-chinese' #'hfl/chinese-roberta-wwm-ext' \n",
        "\n",
        "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                    level=logging.INFO)\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "TqYwvnS7qr9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_name = \"tnews\"\n",
        "# task_name = 'chnsenticorp'\n",
        "max_seq_length = 64\n",
        "train_batch_size = 50\n",
        "num_train_epochs = 5\n",
        "learning_rate = 4e-5\n",
        "cache = \"transformers_cache\"\n",
        "\n",
        "\n",
        "for exp_id in range(1):\n",
        "    data_dir = os.path.join(\"/content/gdrive/My Drive/project/tnews\", \"exp_{}\".format(exp_id))\n",
        "    # data_dir = os.path.join(\"/content/gdrive/My Drive/project/chnsenticorp\", \"exp_{}\".format(exp_id))\n",
        "    output_dir = data_dir\n",
        "    save_model_path = os.path.join(output_dir, 'model')\n",
        "    os.makedirs(save_model_path)\n",
        "    train_pbert_and_augment(task_name, data_dir, output_dir, max_seq_length, train_batch_size, num_train_epochs, learning_rate, cache)\n"
      ],
      "metadata": {
        "id": "SP_aDiQgrYPN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}